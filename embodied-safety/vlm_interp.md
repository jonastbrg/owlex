# VLM Interpretability

**Last Updated**: 2025-02-02
**Status**: Active
**Parent**: [embodied-safety](./STATUS.md)

## Current Focus

Understanding internal representations in vision-language models.

## Recent Progress

- Initial setup complete

## Key Decisions

- (None documented yet)

## Blockers / Risks

- None currently

## Next Steps

1. Define interpretability metrics
2. Set up probing experiments

## Context for Agents

**Location**: `~/embodied-safety/vlm_interp/`
**Tests**: `pytest vlm_interp/tests/`
